{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SBERT Docs Based Code\n",
    "Next, I took a look at the [SBERT documentation](https://sbert.net/). Note that a lot of all code below is copied from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# This is their initial example to show how easy SBERT is\n",
    "# DONE: To know which model to use, take a look at those descriptions: https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6153]])\n",
      "Cosine-Similarity not similar sentence: tensor([[-0.0356]])\n",
      "Cos-Sim of own similar sentences: tensor([[0.3480]])\n",
      "Cos-Sim of own more similar sentences: tensor([[0.8072]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\")\n",
    "emb2 = model.encode(\"Have you seen my red cat?\")\n",
    "emb3 = model.encode(\"I am a student at the university of Antwerp\")\n",
    "emb4 = model.encode(\"Information retrieval is given at our university\")\n",
    "emb5 = model.encode(\"Studying at the university of Antwerp is great\")\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)\n",
    "print(f\"Cosine-Similarity not similar sentence: {util.cos_sim(emb1, emb3)}\")\n",
    "print(f\"Cos-Sim of own similar sentences: {util.cos_sim(emb3, emb4)}\")\n",
    "print(f\"Cos-Sim of own more similar sentences: {util.cos_sim(emb3, emb5)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7553\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7369\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6433\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.2564\n",
      "A man is eating food. \t A man is riding a horse. \t 0.2474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))\n",
    "\n",
    "# TODO: For training own embeddings, I have to take a look here: https://sbert.net/docs/training/overview.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretrained models\n",
    "Currently on [this page](https://sbert.net/docs/pretrained_models.html) of the tutorial.\n",
    "\n",
    "By reading the description of the models, those where the one that seemed interesting to me:\n",
    "- multi-qa-mpnet-base-dot-v1:   This model was tuned for semantic search: Given a query/question, if can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs.\n",
    "- multi-qa-distilbert-cos-v1: \tThis model was tuned for semantic search: Given a query/question, if can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs. (twice as fast as the one above)\n",
    "- multi-qa-MiniLM-L6-cos-v1: \tThis model was tuned for semantic search: Given a query/question, if can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs. (six times as fast as the first one)\n",
    "- all-mpnet-base-v2: All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs.\n",
    "- all-MiniLM-L12-v2: All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs. (the faster version)\n",
    "\n",
    "## Semantic search\n",
    "There are also more models trained specifically on semantic search, which is what we need."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330, 0.0656, 0.9003, 0.6642]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district',\n",
    "                                  'I like trains',\n",
    "                                  'London is very large',\n",
    "                                  'London is bigger than Middelheim'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-QA Models\n",
    "Those where the first three mentioned from my list above and have been trained on 215M question-answer pairs from various sources and domains, including StackExchange, Yahoo Answers, Google & Bing search queries and many more. These model perform well across many search tasks and domains accoording to the site, so I definetly have to try them.\n",
    "\n",
    "#### Bing dataset\n",
    "- msmarco-bert-base-dot-v5\n",
    "\n",
    "Since all our documents are in English, we don't need multi-lingual models.\n",
    "\n",
    "## Pretrained cross-encoders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.152369  -6.2870445]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "scores = model.predict([('Query1', 'Paragraph1'), ('Query1', 'Paragraph2')])\n",
    "\n",
    "#For Example\n",
    "scores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'),\n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "print(scores)\n",
    "# TODO: Take a better look at Retrieve & Re-Rank: https://sbert.net/examples/applications/retrieve_rerank/README.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "# This is not useful for the assignment, but just seemed cool to me (also similar to the tutorials mentioned in the assignment)\n",
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "scores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\n",
    "\n",
    "# Convert scores to labels\n",
    "label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "Take a look [here](https://sbert.net/docs/publications.html) for the references, I think the only required citations I used are the repo and MSMacro.\n",
    "\n",
    "# Computing embeddings\n",
    "I'm currently on [this](https://sbert.net/examples/applications/computing-embeddings/README.html) tutorial page.\n",
    "## Input sequence length\n",
    "As mentioned in one of the earlier tutorial, the longer texts must be truncated.\n",
    ">  Transformer models like BERT / RoBERTa / DistilBERT etc. the runtime and the memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT & Co. are 512 word pieces, which corresponde to about 300-400 words (for English). Longer texts than this are truncated to the first x word pieces.\n",
    "\n",
    "By default, there is a limit of 128 word pieces. Let's take a look at our length distribution graph mentioned in the previous tutorial:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# See how long the text information is\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/training_data.csv\")\n",
    "# Note: Loading the dataframe is put in a different cell to prevent longer loads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. doc_len: 1812.0840857142857\n",
      "max_seq_len: 5148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data number of rows: 140000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df['doc_len'] = df['doc_text'].apply(lambda words: len(str(words).split()))\n",
    "print(f\"avg. doc_len: {df['doc_len'].mean()}\")\n",
    "max_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)\n",
    "print(f\"max_seq_len: {max_seq_len}\")\n",
    "sns.distplot(df['doc_len'], hist=True, kde=True, color='r', label='doc len')\n",
    "plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "plt.title('plot length'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Get an idea of the number of entries:\n",
    "index = df.index\n",
    "number_of_rows = len(index)\n",
    "print(f\"Training data number of rows: {number_of_rows}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the average text length is 1812 words, by adding the standard deviation to this, we get a max_sequence length of 5148 (based on what the previous tutorial described as max sequence length), which is quite long for the models. We can however change the max length for a model, but this will lead to a lot longer times as described above, so this will be a challenge we'll have to face later on. Currently, we'll just assume the first 256 words of a document will contain enough information to determine whether it's relevant for a query.\n",
    "> Note: You cannot increase the length higher than what is maximally supported by the respective transformer model. Also note that if a model was trained on short texts, the representations for long texts might not be that good"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 256\n",
      "Max Sequence Length: 5148\n"
     ]
    }
   ],
   "source": [
    "# Example of how to change the max sequence length\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n",
    "\n",
    "#Change the length to 200\n",
    "model.max_seq_length = 5148\n",
    "\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storing & Loading embeddings\n",
    "When we have made embeddings for all sentences, we can store and load them as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['sentences']\n",
    "    stored_embeddings = stored_data['embeddings']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic search\n",
    "I'm currently at [this](https://sbert.net/examples/applications/semantic-search/README.html) page of the tutorial.\n",
    "As mentioned earlier, it is clear that we're having a case of Assymetric Search:\n",
    "> For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like “What is Python” and you wand to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.\n",
    "\n",
    "The [MsMacro models](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html) should be suitable for asymetric search.\n",
    "\n",
    "Because we're working with longer passages, we'll follow this note and work with the models that are tuned to work with the dot product.\n",
    "> Models tuned for cosine-similarity will prefer the retrieval of shorter passages, while models for dot-product will prefer the retrieval of longer passages. Depending on your task, you might prefer the one or the other type of model.\n",
    "\n",
    "I'll use `msmarco-distilbert-base-tas-b` as model, since this seems the best option for our scenario accoording to the tutorial.\n",
    "\n",
    "Both models are tuned to work with cosine-similarity\n",
    "\n",
    "We've already seen that our training data contains 140.000 rows. Let's take a look at how many documents there are."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wc -l data/all_docs.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 126204 data/all_docs.csv\n",
    "\n",
    "This is a small corpora (less than 1 million entries), so we can probably compute the dot product for all documents.\n",
    "\n",
    "Here's an example for semantic search from the tutorial:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A man is eating pasta.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A man is eating food. (Score: 0.7035)\n",
      "A man is eating a piece of bread. (Score: 0.5272)\n",
      "A man is riding a horse. (Score: 0.1889)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1047)\n",
      "A cheetah is running behind its prey. (Score: 0.0980)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Someone in a gorilla costume is playing a set of drums.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A monkey is playing drums. (Score: 0.6433)\n",
      "A woman is playing violin. (Score: 0.2564)\n",
      "A man is riding a horse. (Score: 0.1389)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1191)\n",
      "A cheetah is running behind its prey. (Score: 0.1080)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A cheetah chases prey on across a field.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A cheetah is running behind its prey. (Score: 0.8253)\n",
      "A man is eating food. (Score: 0.1399)\n",
      "A monkey is playing drums. (Score: 0.1292)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1097)\n",
      "A man is riding a horse. (Score: 0.0650)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    hits = hits[0]      #Get the hits for the first query\n",
    "    for hit in hits:\n",
    "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SBERT also has a `sentence_transformers.util.semantic_search` function built in, which might be a better choice than performing the search manually like the code above.\n",
    "More info can be found [here](https://sbert.net/examples/applications/semantic-search/README.html#sentence_transformers.util.semantic_search).\n",
    "\n",
    "An example of Aproximate Nearest Neighbours (ANN) in case finding the exact top-k is too large, can be found [here](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_quora_hnswlib.py).\n",
    "\n",
    "An example of a retrieve-rerank on the wikipedia paragraphs can be found [here](https://sbert.net/examples/applications/semantic-search/README.html#question-answer-retrieval).\n",
    "\n",
    "## Bi-encoder\n",
    "This maps the query into the same vector space as your documents and loos at the nearest documents.\n",
    "\n",
    "## Cross-encoder\n",
    "Can be used for reranking the (e.g. 100) potentially irrelevant results from the Bi-encoder. This outputs a score between 0 and 1, representing how relevant the document is for the given query.\n",
    "TODO: Continue from here: https://sbert.net/examples/applications/retrieve_rerank/README.html#example-scripts\n",
    "\n",
    "### Example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-11.282532 , -11.377878 ,   2.7110152], dtype=float32)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "model.predict([[\"hey I'm Arno\", \"I study computer science\"],\n",
    "                        [\"hey I'm Arno\", \"some days just make you smile\"],\n",
    "               [\"I like cats\", \"cats are nice\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wikipedia-example\n",
    "This is code based on [the wikipedia example](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb) mentioned in the [tutorial](https://sbert.net/examples/applications/retrieve_rerank/README.html#example-scripts), modified to work on our documents dataset.\n",
    "\n",
    "## Index Bi-Encoder\n",
    "Indexing the Bi-Encoder for semantic search."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11144/1263578065.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Noooooooooooooooooooooooooo, the file doesn't exists :(\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"./data/training_data.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[0mpassages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    486\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    487\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 488\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    489\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1045\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1046\u001B[0m         \u001B[0mnrows\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"nrows\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1047\u001B[1;33m         \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcol_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1048\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1049\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    222\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlow_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m                 \u001B[0mchunks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_low_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    225\u001B[0m                 \u001B[1;31m# destructive to chunks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_concatenate_chunks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchunks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._string_convert\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mc:\\users\\arnod\\pycharmprojects\\neural-retrieval-bert\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers._string_box_utf8\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "#wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "# Nope, we're using our own dataset provided with the assignment\n",
    "all_docs_filepath = \"data/all_docs.csv\"\n",
    "\n",
    "if not os.path.exists(all_docs_filepath):\n",
    "    print(\"Noooooooooooooooooooooooooo, the file doesn't exists :(\")\n",
    "\n",
    "df = pd.read_csv(\"./data/training_data.csv\")\n",
    "\n",
    "passages = []\n",
    "for index, row in df.iterrows():\n",
    "    data = row['doc_text']\n",
    "    first_words = \" \".join(str(data).split()[:500]) # Only take the first 500 words\n",
    "    passages.append(first_words)\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Index Lexical Search\n",
    "We also compare the results to Lexical search, to see the differences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We also compare the results to lexical search (keyword search). Here, we use\n",
    "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Search\n",
    "The search function itself"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function will search all documents for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Sematic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search(\"sustainable agriculture characteristics\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-trained Bi-Encoders (Retrieval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "docs = [\"My first paragraph. That contains information\", \"Python is a programming language.\"]\n",
    "document_embeddings = model.encode(docs)\n",
    "\n",
    "query = \"What is Python?\"\n",
    "query_embedding = model.encode(query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "Great, so far we've only worked with pretrained models, but since we have labeled data, we can fine tune the model with this. I'm continuing the documentation from [here](https://sbert.net/docs/training/overview.html).\n",
    "\n",
    "BERT is used to produce contextualized word embeddings for the input tokens in the text. This is a set of 768 dimensional output vectors. The pooling layer then uses those embeddings to create a vector representing the document. Different options for pooling are possible, for example just taking the mean of all vectors.\n",
    "\n",
    "A SentenceTransformer is the combination of BERT with such a pooling layer, that converts some input sentence(s) into a vector.\n",
    "\n",
    "## Creating a network from scratch\n",
    "The previously used models already combined an embedding model with a pooling layer. You can also define them manually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256) # Texts longer than this will be truncated\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training data\n",
    "SBERT has an InputExample class to store training examples. It accepts pairs of (query, anwer) strings and a label representing semantic similarity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "   InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) # Shuffles the data and creates batches of certain sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Load our own train examples, we can train our data with dev_data.csv\n",
    "from sentence_transformers import InputExample\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/training_data.csv\")\n",
    "\n",
    "train_examples = []\n",
    "for index, row in df.iterrows():\n",
    "    query = str(row['Query'])\n",
    "    doc_text = str(row['doc_text'])\n",
    "    label = float(row['label'])\n",
    "    #first_words = \" \".join(str(data).split()[:500]) # Skip this step, since the embedding model already truncates the data when required\n",
    "    train_examples.append(InputExample(texts=[query, doc_text], label=label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Functiones\n",
    "As mentioned in the tutorial, there is no \"one size fits all\" loss functions, so we'll have to try a bit and see what works on our data.\n",
    "\n",
    "### CosineSimilarityLoss example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d6e6ee7b7be450c9cfe5636cce62f92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d2fbff5c75446ba8736b017d9e6a550"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluators"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9218de2d71cc4ac19b3cd55d249a7a64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8caf9b9dce2e4f358ac10b387f6ddf95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import evaluation\n",
    "sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']\n",
    "sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
    "scores = [0.3, 0.6, 0.2]\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "# ... Your other code to load training data\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Fitting with an evaluator\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator, evaluation_steps=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Continue Training on Other data\n",
    "I've modified the `training_stsbenchmark_continue_training.py` code mentioned in the tutorial to run on our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-19 12:54:25 - Load pretrained SentenceTransformer: nli-distilroberta-base-v2\n",
      "2021-12-19 12:54:33 - Use pytorch device: cuda\n",
      "2021-12-19 12:54:33 - Read the train dataset\n",
      "2021-12-19 12:55:03 - Read dev dataset\n",
      "2021-12-19 12:55:03 - Warmup-steps: 3500\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77dc34d9672a426dadaf6f75603166c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/8750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b8b7f01b264ed0aefb1c42bb8255b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-19 13:01:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1000 steps:\n",
      "2021-12-19 13:05:01 - Cosine-Similarity :\tPearson: 0.2263\tSpearman: 0.2225\n",
      "2021-12-19 13:05:01 - Manhattan-Distance:\tPearson: 0.1984\tSpearman: 0.1951\n",
      "2021-12-19 13:05:01 - Euclidean-Distance:\tPearson: 0.2046\tSpearman: 0.2023\n",
      "2021-12-19 13:05:01 - Dot-Product-Similarity:\tPearson: 0.1818\tSpearman: 0.1826\n",
      "2021-12-19 13:05:01 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 13:11:36 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2000 steps:\n",
      "2021-12-19 13:15:01 - Cosine-Similarity :\tPearson: 0.2538\tSpearman: 0.2517\n",
      "2021-12-19 13:15:01 - Manhattan-Distance:\tPearson: 0.2240\tSpearman: 0.2223\n",
      "2021-12-19 13:15:01 - Euclidean-Distance:\tPearson: 0.2286\tSpearman: 0.2272\n",
      "2021-12-19 13:15:01 - Dot-Product-Similarity:\tPearson: 0.2114\tSpearman: 0.2102\n",
      "2021-12-19 13:15:01 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 13:21:41 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3000 steps:\n",
      "2021-12-19 13:25:12 - Cosine-Similarity :\tPearson: 0.2725\tSpearman: 0.2684\n",
      "2021-12-19 13:25:12 - Manhattan-Distance:\tPearson: 0.2192\tSpearman: 0.2148\n",
      "2021-12-19 13:25:12 - Euclidean-Distance:\tPearson: 0.2297\tSpearman: 0.2259\n",
      "2021-12-19 13:25:12 - Dot-Product-Similarity:\tPearson: 0.2486\tSpearman: 0.2461\n",
      "2021-12-19 13:25:12 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 13:32:18 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 4000 steps:\n",
      "2021-12-19 13:35:47 - Cosine-Similarity :\tPearson: 0.2630\tSpearman: 0.2605\n",
      "2021-12-19 13:35:47 - Manhattan-Distance:\tPearson: 0.2268\tSpearman: 0.2219\n",
      "2021-12-19 13:35:47 - Euclidean-Distance:\tPearson: 0.2395\tSpearman: 0.2358\n",
      "2021-12-19 13:35:47 - Dot-Product-Similarity:\tPearson: 0.2297\tSpearman: 0.2288\n",
      "2021-12-19 13:42:32 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 5000 steps:\n",
      "2021-12-19 13:46:00 - Cosine-Similarity :\tPearson: 0.2785\tSpearman: 0.2747\n",
      "2021-12-19 13:46:00 - Manhattan-Distance:\tPearson: 0.2402\tSpearman: 0.2339\n",
      "2021-12-19 13:46:00 - Euclidean-Distance:\tPearson: 0.2528\tSpearman: 0.2477\n",
      "2021-12-19 13:46:00 - Dot-Product-Similarity:\tPearson: 0.2494\tSpearman: 0.2437\n",
      "2021-12-19 13:46:00 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 13:53:08 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 6000 steps:\n",
      "2021-12-19 13:56:42 - Cosine-Similarity :\tPearson: 0.2928\tSpearman: 0.2880\n",
      "2021-12-19 13:56:42 - Manhattan-Distance:\tPearson: 0.2450\tSpearman: 0.2400\n",
      "2021-12-19 13:56:42 - Euclidean-Distance:\tPearson: 0.2583\tSpearman: 0.2531\n",
      "2021-12-19 13:56:42 - Dot-Product-Similarity:\tPearson: 0.2488\tSpearman: 0.2450\n",
      "2021-12-19 13:56:42 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 14:03:45 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 7000 steps:\n",
      "2021-12-19 14:07:19 - Cosine-Similarity :\tPearson: 0.3032\tSpearman: 0.2980\n",
      "2021-12-19 14:07:19 - Manhattan-Distance:\tPearson: 0.2595\tSpearman: 0.2555\n",
      "2021-12-19 14:07:19 - Euclidean-Distance:\tPearson: 0.2725\tSpearman: 0.2683\n",
      "2021-12-19 14:07:19 - Dot-Product-Similarity:\tPearson: 0.2702\tSpearman: 0.2642\n",
      "2021-12-19 14:07:19 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 14:13:56 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 8000 steps:\n",
      "2021-12-19 14:17:24 - Cosine-Similarity :\tPearson: 0.3049\tSpearman: 0.2996\n",
      "2021-12-19 14:17:24 - Manhattan-Distance:\tPearson: 0.2527\tSpearman: 0.2508\n",
      "2021-12-19 14:17:24 - Euclidean-Distance:\tPearson: 0.2629\tSpearman: 0.2590\n",
      "2021-12-19 14:17:24 - Dot-Product-Similarity:\tPearson: 0.2766\tSpearman: 0.2723\n",
      "2021-12-19 14:17:24 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 14:22:25 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 0:\n",
      "2021-12-19 14:25:58 - Cosine-Similarity :\tPearson: 0.3047\tSpearman: 0.2990\n",
      "2021-12-19 14:25:58 - Manhattan-Distance:\tPearson: 0.2512\tSpearman: 0.2437\n",
      "2021-12-19 14:25:58 - Euclidean-Distance:\tPearson: 0.2659\tSpearman: 0.2584\n",
      "2021-12-19 14:25:58 - Dot-Product-Similarity:\tPearson: 0.2811\tSpearman: 0.2715\n"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/8750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01dec3f0f7da4cf583a2608d9931da13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-19 14:32:43 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 1000 steps:\n",
      "2021-12-19 14:36:18 - Cosine-Similarity :\tPearson: 0.3033\tSpearman: 0.3003\n",
      "2021-12-19 14:36:18 - Manhattan-Distance:\tPearson: 0.2564\tSpearman: 0.2523\n",
      "2021-12-19 14:36:18 - Euclidean-Distance:\tPearson: 0.2678\tSpearman: 0.2625\n",
      "2021-12-19 14:36:18 - Dot-Product-Similarity:\tPearson: 0.2852\tSpearman: 0.2798\n",
      "2021-12-19 14:36:18 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 14:43:18 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 2000 steps:\n",
      "2021-12-19 14:46:49 - Cosine-Similarity :\tPearson: 0.3050\tSpearman: 0.3009\n",
      "2021-12-19 14:46:49 - Manhattan-Distance:\tPearson: 0.2763\tSpearman: 0.2701\n",
      "2021-12-19 14:46:49 - Euclidean-Distance:\tPearson: 0.2861\tSpearman: 0.2790\n",
      "2021-12-19 14:46:49 - Dot-Product-Similarity:\tPearson: 0.2861\tSpearman: 0.2812\n",
      "2021-12-19 14:46:49 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 14:53:36 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 3000 steps:\n",
      "2021-12-19 14:57:22 - Cosine-Similarity :\tPearson: 0.3101\tSpearman: 0.3074\n",
      "2021-12-19 14:57:22 - Manhattan-Distance:\tPearson: 0.2805\tSpearman: 0.2743\n",
      "2021-12-19 14:57:22 - Euclidean-Distance:\tPearson: 0.2953\tSpearman: 0.2883\n",
      "2021-12-19 14:57:22 - Dot-Product-Similarity:\tPearson: 0.2898\tSpearman: 0.2839\n",
      "2021-12-19 14:57:22 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 15:04:20 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 4000 steps:\n",
      "2021-12-19 15:08:05 - Cosine-Similarity :\tPearson: 0.3106\tSpearman: 0.3066\n",
      "2021-12-19 15:08:05 - Manhattan-Distance:\tPearson: 0.2786\tSpearman: 0.2712\n",
      "2021-12-19 15:08:05 - Euclidean-Distance:\tPearson: 0.2926\tSpearman: 0.2847\n",
      "2021-12-19 15:08:05 - Dot-Product-Similarity:\tPearson: 0.3007\tSpearman: 0.2940\n",
      "2021-12-19 15:15:13 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 5000 steps:\n",
      "2021-12-19 15:18:58 - Cosine-Similarity :\tPearson: 0.3174\tSpearman: 0.3138\n",
      "2021-12-19 15:18:58 - Manhattan-Distance:\tPearson: 0.2718\tSpearman: 0.2657\n",
      "2021-12-19 15:18:58 - Euclidean-Distance:\tPearson: 0.2899\tSpearman: 0.2834\n",
      "2021-12-19 15:18:58 - Dot-Product-Similarity:\tPearson: 0.3039\tSpearman: 0.2988\n",
      "2021-12-19 15:18:58 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 15:25:52 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 6000 steps:\n",
      "2021-12-19 15:29:27 - Cosine-Similarity :\tPearson: 0.3233\tSpearman: 0.3175\n",
      "2021-12-19 15:29:27 - Manhattan-Distance:\tPearson: 0.2731\tSpearman: 0.2641\n",
      "2021-12-19 15:29:27 - Euclidean-Distance:\tPearson: 0.2867\tSpearman: 0.2768\n",
      "2021-12-19 15:29:27 - Dot-Product-Similarity:\tPearson: 0.3114\tSpearman: 0.3069\n",
      "2021-12-19 15:29:27 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 15:36:35 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 7000 steps:\n",
      "2021-12-19 15:40:11 - Cosine-Similarity :\tPearson: 0.3251\tSpearman: 0.3211\n",
      "2021-12-19 15:40:11 - Manhattan-Distance:\tPearson: 0.2910\tSpearman: 0.2827\n",
      "2021-12-19 15:40:11 - Euclidean-Distance:\tPearson: 0.3076\tSpearman: 0.2997\n",
      "2021-12-19 15:40:11 - Dot-Product-Similarity:\tPearson: 0.3123\tSpearman: 0.3051\n",
      "2021-12-19 15:40:12 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 15:46:59 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 1 after 8000 steps:\n",
      "2021-12-19 15:50:08 - Cosine-Similarity :\tPearson: 0.3332\tSpearman: 0.3309\n",
      "2021-12-19 15:50:08 - Manhattan-Distance:\tPearson: 0.3039\tSpearman: 0.2951\n",
      "2021-12-19 15:50:08 - Euclidean-Distance:\tPearson: 0.3200\tSpearman: 0.3118\n",
      "2021-12-19 15:50:08 - Dot-Product-Similarity:\tPearson: 0.3137\tSpearman: 0.3079\n",
      "2021-12-19 15:50:08 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2021-12-19_12-54-25\n",
      "2021-12-19 15:55:01 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 1:\n",
      "2021-12-19 15:58:19 - Cosine-Similarity :\tPearson: 0.3324\tSpearman: 0.3297\n",
      "2021-12-19 15:58:19 - Manhattan-Distance:\tPearson: 0.2914\tSpearman: 0.2838\n",
      "2021-12-19 15:58:19 - Euclidean-Distance:\tPearson: 0.3110\tSpearman: 0.3044\n",
      "2021-12-19 15:58:19 - Dot-Product-Similarity:\tPearson: 0.3178\tSpearman: 0.3137\n"
     ]
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/8750 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d226994c26843fd93f79a72d2fc8239"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-19 16:04:20 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 2 after 1000 steps:\n",
      "2021-12-19 16:07:26 - Cosine-Similarity :\tPearson: 0.3260\tSpearman: 0.3239\n",
      "2021-12-19 16:07:26 - Manhattan-Distance:\tPearson: 0.3012\tSpearman: 0.2944\n",
      "2021-12-19 16:07:26 - Euclidean-Distance:\tPearson: 0.3160\tSpearman: 0.3096\n",
      "2021-12-19 16:07:26 - Dot-Product-Similarity:\tPearson: 0.3179\tSpearman: 0.3135\n",
      "2021-12-19 16:13:25 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 2 after 2000 steps:\n",
      "2021-12-19 16:16:45 - Cosine-Similarity :\tPearson: 0.3187\tSpearman: 0.3149\n",
      "2021-12-19 16:16:45 - Manhattan-Distance:\tPearson: 0.2883\tSpearman: 0.2798\n",
      "2021-12-19 16:16:45 - Euclidean-Distance:\tPearson: 0.3055\tSpearman: 0.2970\n",
      "2021-12-19 16:16:45 - Dot-Product-Similarity:\tPearson: 0.3124\tSpearman: 0.3059\n",
      "2021-12-19 16:41:30 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 2 after 3000 steps:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'nli-distilroberta-base-v2' from the server.\n",
    "It then fine-tunes this model for some epochs on the provided dev_data dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "Note: This code was for semantic textual similarity, which doesn't really apply to our dataset. Another base model would've been better for our dataset.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Read the dataset\n",
    "model_name = 'nli-distilroberta-base-v2'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read the train dataset\")\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "def get_sample_list(file_path, test_data=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    samples = []\n",
    "    label_score = 0.99\n",
    "    previous_query = None\n",
    "    for index, row in df.iterrows():\n",
    "        query = str(row['Query'])\n",
    "        doc_text = str(row['doc_text'])\n",
    "\n",
    "        # Test data doesn't have a label, so we have to improvise here\n",
    "        if not test_data:\n",
    "            label = float(row['label'])\n",
    "        else:\n",
    "            if previous_query == query:\n",
    "                label_score *= 0.99 # Later result for the same query, so the score gets a little bit lower\n",
    "            else:\n",
    "                label_score = 0.99\n",
    "            previous_query = query\n",
    "            label = label_score\n",
    "\n",
    "        inp_example = InputExample(texts=[query, doc_text], label=label)\n",
    "\n",
    "        samples.append(inp_example)\n",
    "    return samples\n",
    "\n",
    "train_samples = get_sample_list(\"./data/training_data.csv\")\n",
    "dev_samples = get_sample_list(\"./data/dev_data.csv\")\n",
    "test_samples = get_sample_list(\"./data/test_data.csv\", test_data=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Development set: Measure correlation between cosine score and gold labels\n",
    "logging.info(\"Read dev dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "# Load the stored model and evaluate its performance on the test dataset\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see the model improving the first steps, but there aren't really significant steps made. Both the Pearson and Speaman coefficient are not close to zero, so there is indeed some sort of lineair correlation, that overall improves when training the model more.\n",
    "Extra information about the pearson and spearman coefficients have I found [here](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/).\n",
    "\n",
    "The model also takes a lot of time to train. This isn't such a large problem, since you only have to train (fine tune) the model once and can then reuse it. The only problem with this is that it doesn't give us space to experiment with a lot of different models.\n",
    "\n",
    "## Training CrossEncoders\n",
    "We are working with a continious score, since we wan't to determine the relevance and \"somewhat-relevant\" should also be an option with different degrees of relevance. This means that when loading our CrossEncoder, the parameter num_labels should be set to 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', num_labels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmented SBERT\n",
    "Currently on [this page](https://sbert.net/examples/training/data_augmentation/README.html) of the tutorial.\n",
    "You can extend your dataset with some augmented data, but with the size of our current dataset, we have already enough and training already takes a lot of time, so we won't generate extra augmented data.\n",
    "\n",
    "## Loss function\n",
    "We can try to use MultipleNegativesRankingLoss, since this should be useful for information retrieval / semantic search. More info can be found [here](https://sbert.net/examples/training/quora_duplicate_questions/README.html#multiplenegativesrankingloss)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code copy pasted from the tutorial to show how to set MultipleNegativesRankingLoss for future reference\n",
    "train_samples = []\n",
    "with open(os.path.join(dataset_path, \"classification/train_pairs.tsv\"), encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        # We should only add if the label is 0 (meaning irrelevant, so a negative pair)\n",
    "        if row['is_duplicate'] == '1':\n",
    "            # And our model is Assymmetric, so we should only add it in one direction\n",
    "            train_samples.append(InputExample(texts=[row['question1'], row['question2']], label=1))\n",
    "            train_samples.append(InputExample(texts=[row['question2'], row['question1']], label=1)) #if A is a duplicate of B, then B is a duplicate of A\n",
    "\n",
    "\n",
    "# After reading the train_samples, we create a SentencesDataset and a DataLoader\n",
    "train_dataset = SentencesDataset(train_samples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that it's also possible to combine multiple losses. Here also the code from the tutorial for this copy pasted for future reference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_samples_MultipleNegativesRankingLoss = []\n",
    "train_samples_ConstrativeLoss = []\n",
    "\n",
    "with open(os.path.join(dataset_path, \"classification/train_pairs.tsv\"), encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        train_samples_ConstrativeLoss.append(InputExample(texts=[row['question1'], row['question2']], label=int(row['is_duplicate'])))\n",
    "        if row['is_duplicate'] == '1':\n",
    "            train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[row['question1'], row['question2']], label=1))\n",
    "            train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[row['question2'], row['question1']], label=1))  # if A is a duplicate of B, then B is a duplicate of A\n",
    "\n",
    "# Create data loader and loss for MultipleNegativesRankingLoss\n",
    "train_dataset_MultipleNegativesRankingLoss = SentencesDataset(train_samples_MultipleNegativesRankingLoss, model=model)\n",
    "train_dataloader_MultipleNegativesRankingLoss = DataLoader(train_dataset_MultipleNegativesRankingLoss, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss_MultipleNegativesRankingLoss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "\n",
    "# Create data loader and loss for OnlineContrastiveLoss\n",
    "train_dataset_ConstrativeLoss = SentencesDataset(train_samples_ConstrativeLoss, model=model)\n",
    "train_dataloader_ConstrativeLoss = DataLoader(train_dataset_ConstrativeLoss, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss_ConstrativeLoss = losses.OnlineContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n",
    "\n",
    "# .....\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader_MultipleNegativesRankingLoss, train_loss_MultipleNegativesRankingLoss), (train_dataloader_ConstrativeLoss, train_loss_ConstrativeLoss)],\n",
    "          evaluator=seq_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=1000,\n",
    "          output_path=model_save_path\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training MS MARCO\n",
    "As mentioned earlier, MS Marco seems the ideal dataset for this assignment. Luky me, the tutorial also has [a page](https://sbert.net/examples/training/ms_marco/README.html) describing how to fine tune this one.\n",
    "\n",
    "With MultipleNegativeRankingLoss, we should create triples (query, relevant_text, non_relevant_text).\n",
    "\n",
    "> One way to improve training is to choose really good negatives, also know as hard negative: The negative should look really similar to the positive passage, but it should not be relevant to the query.\n",
    "\n",
    "Since the data we got was labeled by users, the negative labeled items were probably recommended using a different algorithm, so it's indead similar to the positives, but not relevant for the query. They also recommend finding the pairs by using a Cross-Encoder on the results provided by a Bi-Encoder.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}