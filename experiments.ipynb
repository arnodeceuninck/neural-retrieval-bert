{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "In order to experiment with new models, we first need a way to evaluate the results we achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We're using a document retriever class, to make it easier to perform the evaluator code on different models\n",
    "class DocumentRetriever:\n",
    "    def __init__(self):\n",
    "        self.all_docs = pd.read_csv(\"data/all_docs.csv\")\n",
    "        self.all_queries = pd.read_csv(\"data/dev_queries.csv\")\n",
    "\n",
    "    def retrieve_documents(self, query_number, n):\n",
    "        # Return the n best recommendations (ordered by decreasing relevance) for given query\n",
    "        assert Exception(\"Function not implemented in subclass\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[476602, 432658, 429474, 346632, 122086, 362869, 60461, 417115, 29215, 467667]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LuceneRetriever(DocumentRetriever):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lucene_raw_retrievals = pd.read_csv(\"data/raw_dev_Lucene_retrievals.csv\")\n",
    "\n",
    "    def retrieve_documents(self, query_number, n):\n",
    "        relevant_docs = self.lucene_raw_retrievals[self.lucene_raw_retrievals['Query_number'] == query_number]\n",
    "        top_n = relevant_docs.head(n)\n",
    "        return top_n['doc_number'].to_list()\n",
    "\n",
    "luceneRetriever = LuceneRetriever()\n",
    "results = luceneRetriever.retrieve_documents(1089071, 10)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GroundTruthRetriever(DocumentRetriever):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ground_truth = pd.read_csv(\"data/dev_data.csv\")\n",
    "\n",
    "    def retrieve_documents(self, query_number, n=None):\n",
    "        assert n is None # The truth has no limits, it just retrieves all relevant documents\n",
    "        relevant_docs = self.ground_truth[self.ground_truth['Query_number'] == query_number]\n",
    "        return relevant_docs['doc_number'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ModelRater:\n",
    "    def __init__(self):\n",
    "        self.ground_truth_retriever = GroundTruthRetriever()\n",
    "        self.probe_data = pd.read_csv(\"data/dev_queries.csv\")\n",
    "\n",
    "    def single_query_recall_precission(self, query_nr, n, retriever):\n",
    "        query_results = set(retriever.retrieve_documents(query_nr, n))\n",
    "        ground_truth = set(self.ground_truth_retriever.retrieve_documents(query_nr))\n",
    "\n",
    "        # Some of this code is based on https://stackoverflow.com/questions/55952408/how-to-calculate-precision-and-recall-of-2-lists-in-python\n",
    "        intersect_length = len(query_results.intersection(ground_truth))\n",
    "        precision = intersect_length/len(query_results)\n",
    "        recall = intersect_length/len(ground_truth)\n",
    "\n",
    "        # print(f\"R({recall}), P({precision})\")\n",
    "\n",
    "        return recall, precision\n",
    "\n",
    "\n",
    "    def get_rating(self, retriever: DocumentRetriever, n=10) -> (float, float):\n",
    "        # returns the recall, precision\n",
    "\n",
    "        precision_sum = 0\n",
    "        recall_sum = 0\n",
    "        query_count = 0\n",
    "        # For every query in probe data, determine the results\n",
    "        for counter, query in tqdm(self.probe_data.iterrows(), total=self.probe_data.shape[0]):\n",
    "            query_nr = query['Query_number']\n",
    "\n",
    "            recall, precision = self.single_query_recall_precission(query_nr, n, retriever)\n",
    "\n",
    "            precision_sum += precision\n",
    "            recall_sum += recall\n",
    "            query_count += 1\n",
    "\n",
    "        avg_precision = precision_sum/query_count\n",
    "        avg_recall = recall_sum/query_count\n",
    "\n",
    "        print(f\"Recall: {avg_recall}, Precision: {avg_precision}\")\n",
    "\n",
    "        return avg_recall, avg_precision\n",
    "\n",
    "rater = ModelRater()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/644 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11698347e9b44ad897f0eff5ae5c7d9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.32744769762105247, Precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "recall, precision = rater.get_rating(luceneRetriever)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's strange that the precision is always exactly 1.0. This is probably because the ground truth results (dev_data.csv) is based on the lucene retrievals (raw_dev_Lucene_retrievals.csv).\n",
    "\n",
    "## BM25 retrieval\n",
    "Another lexical search, which was mentioned in the SBERT-Documentation.\n",
    "\n",
    "We first define a helper functions to get the text given a query number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'how does sperm develop'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_queries = pd.read_csv('data/dev_queries.csv')\n",
    "def query_nr_to_text(query_nr):\n",
    "    return all_queries[all_queries['Query_number'] == query_nr]['Query'].iat[0]\n",
    "query_nr_to_text(1099178)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/126203 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb3fb78184f24b4a82b759d7581e40ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/644 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "946989c5116c4f60b27b2e002e62ec8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.24225742435777975, Precision: 0.9601671103223895\n"
     ]
    }
   ],
   "source": [
    "# We also compare the results to lexical search (keyword search). Here, we use\n",
    "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "# from tqdm.notebook import tqdm # or autonotebook?\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class Bm25Retriever(DocumentRetriever):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        all_docs = pd.read_csv('data/all_docs.csv')\n",
    "\n",
    "        self.reverse_index = []\n",
    "        self.tokenized_corpus = []\n",
    "        for count, row in tqdm(all_docs.iterrows(), total=all_docs.shape[0]):\n",
    "            # Work with a smaller dataset when debugging\n",
    "            # if len(self.reverse_index) == 1000:\n",
    "            #     break\n",
    "            passage = str(row['doc_text'])\n",
    "            doc_nr = row['doc_number']\n",
    "            self.tokenized_corpus.append(self.bm25_tokenizer(passage))\n",
    "            self.reverse_index.append(doc_nr)\n",
    "\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    # We lower case our text and remove stop-words from indexing\n",
    "    @staticmethod\n",
    "    def bm25_tokenizer(text):\n",
    "        tokenized_doc = []\n",
    "        for token in text.lower().split():\n",
    "            token = token.strip(string.punctuation)\n",
    "\n",
    "            if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "                tokenized_doc.append(token)\n",
    "        return tokenized_doc\n",
    "\n",
    "\n",
    "     ##### BM25 search (lexical search) #####\n",
    "    def search(self, query_text, n):\n",
    "        # This code is based on the code from sbert-doc.ipynb\n",
    "        bm25_scores = self.bm25.get_scores(self.bm25_tokenizer(query_text))\n",
    "        top_n = np.argpartition(bm25_scores, -n)[-n:]\n",
    "        bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "        bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        results = []\n",
    "        for hit in bm25_hits:\n",
    "            results.append(self.reverse_index[hit['corpus_id']])\n",
    "        return results\n",
    "\n",
    "\n",
    "    def retrieve_documents(self, query_number, n):\n",
    "        return self.search(query_nr_to_text(query_number), n)\n",
    "\n",
    "bm25Retriever = Bm25Retriever()\n",
    "recall, precision = rater.get_rating(bm25Retriever)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MsMarco\n",
    "See what's the precision on the pretrained MsMarco bi-encoder and cross-encoder. This code was based on code from the sbert-doc.ipynb notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super inited\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/126203 [00:00<00:54, 2317.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 51\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff766125dfd7438583592c3a349faa23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 644/644 [00:47<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.0003867636647012301, Precision: 0.0013975155279503104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BiCrossRetriever(DocumentRetriever):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Super inited\")\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "        #We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "        self.bi_encoder = SentenceTransformer('msmarco-bert-base-dot-v5')\n",
    "        self.bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "        self.top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "        #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "\n",
    "        self.passages = []\n",
    "        self.reverse_indices = []\n",
    "        for index, row in tqdm(self.all_docs.iterrows(), total=self.all_docs.shape[0]):\n",
    "            if len(self.passages) > 50:\n",
    "                # Use a very small subset when debugging\n",
    "                break\n",
    "            data = row['doc_text']\n",
    "            first_words = \" \".join(str(data).split()[:500]) # Only take the first 500 words\n",
    "            self.passages.append(first_words)\n",
    "            self.reverse_indices.append(row['doc_number'])\n",
    "\n",
    "        print(\"Passages:\", len(self.passages))\n",
    "\n",
    "        # We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "        self.corpus_embeddings = self.bi_encoder.encode(self.passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    def get_bi_encoder_hits(self, query):\n",
    "        question_embedding = self.bi_encoder.encode(query, convert_to_tensor=True)\n",
    "        question_embedding = question_embedding.cuda()\n",
    "        hits = util.semantic_search(question_embedding, self.corpus_embeddings, top_k=self.top_k)\n",
    "        hits = hits[0]  # Get the hits for the first query\n",
    "        return hits\n",
    "\n",
    "    def retrieve_documents(self, query_nr, n):\n",
    "        # Find potentially relevant passages with bi_encoder\n",
    "        query = query_nr_to_text(query_nr)\n",
    "        hits = self.get_bi_encoder_hits(query)\n",
    "\n",
    "        # Re-rank those matches with the cross-encoder\n",
    "        cross_inp = [[query, self.passages[hit['corpus_id']]] for hit in hits]\n",
    "        cross_scores = self.cross_encoder.predict(cross_inp)\n",
    "\n",
    "        # Sort results by the cross-encoder scores\n",
    "        for idx in range(len(cross_scores)):\n",
    "            hits[idx]['cross-score'] = cross_scores[idx]\n",
    "        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "\n",
    "        results = []\n",
    "        for hit in hits[0:n]:\n",
    "            results.append(self.reverse_indices[hit['corpus_id']])\n",
    "\n",
    "        return results\n",
    "\n",
    "biCrossRetriever = BiCrossRetriever()\n",
    "recall, precision = rater.get_rating(biCrossRetriever)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/644 [06:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10692/510794106.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[0mBiCrossRetriever\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieve_documents\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbi_cross_retriever_retrieve_documents\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m \u001B[0mrecall\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrater\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_rating\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbiCrossRetriever\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10692/3598500002.py\u001B[0m in \u001B[0;36mget_rating\u001B[1;34m(self, retriever, n)\u001B[0m\n\u001B[0;32m     30\u001B[0m             \u001B[0mquery_nr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mquery\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Query_number'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m             \u001B[0mrecall\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprecision\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msingle_query_recall_precission\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquery_nr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretriever\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m             \u001B[0mprecision_sum\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mprecision\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10692/3598500002.py\u001B[0m in \u001B[0;36msingle_query_recall_precission\u001B[1;34m(self, query_nr, n, retriever)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msingle_query_recall_precission\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mquery_nr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretriever\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[0mquery_results\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mretriever\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieve_documents\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquery_nr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m         \u001B[0mground_truth\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mground_truth_retriever\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieve_documents\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquery_nr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10692/510794106.py\u001B[0m in \u001B[0;36mbi_cross_retriever_retrieve_documents\u001B[1;34m(self, query_nr, n)\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[0mhits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'cross-score'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mhits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhits\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'cross-score'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreverse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m     \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10692/510794106.py\u001B[0m in \u001B[0;36mbi_cross_retriever_retrieve_documents\u001B[1;34m(self, query_nr, n)\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[0mhits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'cross-score'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mhits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhits\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'cross-score'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreverse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhits\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m     \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2021.2.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1146\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1147\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2021.2.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1160\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1162\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "Some additional experiments. Be sure to take a look at `sbert-doc.ipynb` before this.\n",
    "\n",
    "This is the same as the previous example, but by using the MsMacro dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\" # Since GPU 0 won't work on my laptop\n",
    "# import torch\n",
    "# available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "# available_gpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.empty_cache()\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Read the dataset\n",
    "model_name = 'msmarco-distilbert-base-tas-b'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-' + model_name + '-' + datetime.now().strftime(\n",
    "    \"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read the train dataset\")\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_sample_list(file_path, test_data=False):\n",
    "    df = pd.read_csv(file_path)\n",
    "    samples = []\n",
    "    label_score = 0.99\n",
    "    previous_query = None\n",
    "    for index, row in df.iterrows():\n",
    "        query = str(row['Query'])\n",
    "        doc_text = str(row['doc_text'])\n",
    "\n",
    "        # Test data doesn't have a label, so we have to improvise here\n",
    "        if not test_data:\n",
    "            label = float(row['label'])\n",
    "        else:\n",
    "            if previous_query == query:\n",
    "                label_score *= 0.99  # Later result for the same query, so the score gets a little bit lower\n",
    "            else:\n",
    "                label_score = 0.99\n",
    "            previous_query = query\n",
    "            label = label_score\n",
    "\n",
    "        inp_example = InputExample(texts=[query, doc_text], label=label)\n",
    "\n",
    "        samples.append(inp_example)\n",
    "    return samples\n",
    "\n",
    "\n",
    "train_samples = get_sample_list(\"./data/training_data.csv\")\n",
    "dev_samples = get_sample_list(\"./data/dev_data.csv\")\n",
    "test_samples = get_sample_list(\"./data/test_data.csv\", test_data=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Development set: Measure correlation between cosine score and gold labels\n",
    "logging.info(\"Read dev dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "# Load the stored model and evaluate its performance on the test dataset\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}